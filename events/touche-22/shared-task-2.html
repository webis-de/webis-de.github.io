---
layout: workshop
nav_active: shared task 2
title: Touché @ CLEF
description: Touché @ CLEF Argument Retrieval
shared_tasks: 3
---
<nav class="uk-container">
<ul class="uk-breadcrumb">
<li><a href="https://webis.de/index.html">Webis.de</a></li>
<li><a href="https://webis.de/events.html">Events</a></li>
<li><a href="index.html">Touché 2022</a></li>
<li class="uk-disabled"><a href="#">Task 2</a></li>
</ul>
</nav>

<script type="application/ld+json">
{
    "@context":"http://schema.org/",
    "@type":"Shared Task",
    "name":"Touché @ CLEF",
    "description":"Touché @ CLEF Argument Retrieval",
    "text":"*Shared Task, any other text",
    "url":"*link that leads to this HTML web page.",
    "sameAs":"*is there an id link, e.g. from zenodo?",
    "keywords":[
        "touché", 
        "argument retrieval", 
        "argument retrieval for controversial questions", 
        "argument retrieval for comparative questions"
    ], 
    "creator":[
        {
            "@type":"Organization",
            "url":"https://webis.de/",
            "name":"The Web Technology & Information Systems Network",
            "alternateName":"Webis"
        },
        {
            "@id":"*orcid id if available", 
            "@type":"Person", 
            "url":"*https://www.uni-weimar.de/en/media/chairs/computer-science-department/webis/people/#name",
            "affiliation":"*Bauhaus-Universit\u00e4t Weimar", 
            "name":"*last name, first name"
        },
        {
            "@id":"*orcid id if available", 
            "@type":"Person", 
            "url":"*https://www.uni-weimar.de/en/media/chairs/computer-science-department/webis/people/#name",
            "affiliation":"*Bauhaus-Universit\u00e4t Weimar", 
            "name":"*last name, first name"
        }
    ],
    "includedInDataCatalog":{
    },
    "distribution":[
    ]
}
</script>

<main class="uk-section uk-section-default">
    <div class="uk-container">
        <h1>Touché Task 2: Argument Retrieval for Comparative Questions</h1>

        <ul class="uk-list">
            <!-- Comment out sections you do not provide -->
            <li><span data-uk-icon="chevron-down"></span> <a href="#synopsis">Synopsis</a></li>
            <li><span data-uk-icon="chevron-down"></span> <a href="#task">Task</a></li>
            <li><span data-uk-icon="chevron-down"></span> <a href="#data">Data</a></li>
            <li><span data-uk-icon="chevron-down"></span> <a href="#evaluation">Evaluation</a></li>
            <li><span data-uk-icon="chevron-down"></span> <a href="#submission">Submission</a></li>
            <li><span data-uk-icon="chevron-down"></span> <a href="#tira-quickstart">TIRA Quickstart</a></li>
 	    <li><span data-uk-icon="chevron-down"></span> <a href="#results">Results</a></li>
            <li><span data-uk-icon="chevron-down"></span> <a href="#task-committee">Task Committee</a></li>
        </ul>
    </div>

    <div class="uk-container uk-margin-medium">
        <!--
        SECTION Synopsis
        -->
        <h2 id="synopsis">Synopsis</h2>

        <ul>
            <li>Task: Given a comparative topic and a collection of documents, the task is to retrieve relevant argumentative passages for either compared object or for both and to detect their respective stances with respect to the object they talk about.</li>
            <li>Input: [<a href="https://files.webis.de/corpora/corpora-webis/corpus-touche-task2-22/touche-task2-passages-version-002.jsonl.gz">document collection</a>] [<a href="data/task2-stance-dataset.zip">stance dataset</a>]</li>
            <li>Submission: [<a href="https://www.tira.io/task/touche-task-2/">submit</a>]</li>
            <li>Manual judgments (qrels):  [<a href="https://files.webis.de/corpora/corpora-webis/corpus-touche-task2-22/touche-task2-2022-relevance.qrels">relevance</a>], [<a href="https://files.webis.de/corpora/corpora-webis/corpus-touche-task2-22/touche-task2-2022-quality.qrels">quality</a>], [<a href="https://files.webis.de/corpora/corpora-webis/corpus-touche-task2-22/touche-task2-2022-stance.qrels">stance</a>] (top-5 pooling)</li>
        </ul>

        <!--<p>You may provide links to associated resources, like so: [<a>link</a>]</p>-->


        <!--
        SECTION Task
        -->
        <h2 id="task">Task</h2>

        <p>The goal of <strong>Task 2</strong> is to support users facing some choice problem from "everyday life". Given a comparative topic and a collection of documents, the task is to retrieve relevant argumentative passages for either compared object or for both and to detect their respective stances with respect to the object they talk about. </p>
        
        <a class="uk-button uk-button-primary" href="https://clef2022-labs-registration.dei.unipd.it/">Register now</a>
<!--
                <a class="uk-button uk-button-primary" href="http://clef2020-labs-registration.dei.unipd.it/registrationForm.php">Register now</a>
-->
		
	<!--
        SECTION Data
        -->
        <h2 id="data">Data</h2>
        <p><strong>Touché 2022 topics</strong> for Task 2 can be <a href="data/topics-task2-2022.zip">downloaded from here</a> (50 topics). The topics are provided as an XML file. The notebook for parsing the topics can be <a href="code/parse_topics.ipynb">downloaded from here</a>. <br><br>

                        Example topic for <strong>Task 2:</strong><br><br>
                        &nbsp;&nbsp;&nbsp;&#60;topic&#62;<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#60;number&#62;25&#60;/number&#62;<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#60;title&#62;Which browser is better, Internet Explorer or Firefox?&#60;/title&#62;<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#60;objects&#62;Internet Explorer, Firefox&#60;/objects&#62;<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#60;description&#62;A windows user wants to decide whether to use the default Internet Explorer or go for Firefox as alternative and seeks for information to compare them in multiple different aspects such as security and privacy, performance, ease of use, reliability, etc.&#60;/description&#62;<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#60;narrative&#62;Highly relevant documents discuss in detail pros and cons of both web browsers by contrasting their characteristics and features. This comparison may contain technical details but also personal opinions. A relevant document may explicitly state what are the good/bad features of either the Internet Explorer or the Firefox browser without a clear comparison. Documents that include only descriptions of browser war or historical background how the browsers evolved, etc. are not relevant.&#60;/narrative&#62;<br>
                        &nbsp;&nbsp;&nbsp;&#60;/topic&#62;<br><br></p>
                        
        <p><strong>The corpus</strong> for Task 2 is the <strong>collection</strong> [<a href="https://files.webis.de/corpora/corpora-webis/corpus-touche-task2-22/touche-task2-passages-version-002.jsonl.gz">download</a>] of about 0.9 million text passages.</p>
        
        <h3>Additional Resources</h3>
        
        <ul> 
        <!--<li>MS MARCO questions only [<a href="https://files.webis.de/corpora/corpora-webis/corpus-touche-task2-22/msmarco_questions_classified.tsv">download</a>] classified as comparative or not.</li>-->
        <li>Subset of MS MARCO [<a href="https://files.webis.de/corpora/corpora-webis/corpus-touche-task2-22/marco_comp_all_fields.tsv">download</a>] with comparative questions.</li>
        <li><strong>Collection</strong> [<a href="https://files.webis.de/corpora/corpora-webis/corpus-touche-task2-22/touche-task2-passages-version-002-expanded-with-doc-t5-query.jsonl.gz">download</a>] of text passages expanded with queries generated using DocT5Query.</li>        
        </ul>     
                        
        <h3>Touché 2021 Results</h3>
        
        [<a href="./topics-task-2-2021.zip">Download full topics Touché 2021</a>]<br>                
        [<a href="./touche-task2-51-100-relevance.qrels">Download relevance judgments from Touché 2021</a>]<br>
        [<a href="./touche-task2-51-100-quality.qrels">Download quality judgments from Touché 2021</a>]<br>
        
        <h3>Touché 2020 Results</h3>
                        
        [<a href="./topics-task-2-2020.zip">Download full topics from Touché 2020</a>]<br>
        [<a href="./touche2020-task2-relevance-withbaseline.qrels">Download relevance judgments from Touché 2020</a>]<br>
        [<a href="./runs-task-2-2020.zip">Download submitted ranked documents (runs) from Touché 2020</a>]<br><br>

	<!--
        <h3 class="uk-margin-small-top">Output Format</h3>
        <p>Output format description.</p>
	-->

        <!--
        SECTION Evaluation
        -->
        <h2 id="evaluation">Evaluation</h2>
        <p>Be sure to retrieve relevant documents that comprise convincing argumentation for or against one option or the other. Our human assessors will label the retrieved documents manually, both for their general topical relevance and for the rhetorical quality, i.e., "well-writtenness" of the document: (1) whether a document contains arguments (i.e., argumentative support is provided) and whether the text has a good style of speech (formal language is preferred over informal), (2) whether the text has a proper sentence structure and is easy to read and follow, whether it can be well understood, (3) whether it includes profanity, has typos, and makes use of other detrimental style choices.</p>
        
        <p>Additionally, classify the stance of the retrieved text passages towards the compared objects in questions. For instance, in the question <i>Who is a better friend, a cat or a dog?</i>  the terms <i>cat</i> and <i>dog</i> are the comparison objects. An answer candidate like <i>Cats can be quite affectionate and attentive, and thus are good friends</i> should be classified as pro the <i>cat</i> object, while <i>Cats are less faithful than dogs</i> as supporting the <i>dog</i> object.</p>
        
	The format of the relevance / quality judgment files from Touché 2020 and 2021:<br><br>
        <code>qid 0 doc rel</code><br><br>
        With:
		<ul>
		        <li><code>qid</code>: The topic number.</li>
		        <li><code>0</code>: Unused, always 0.</li>
		        <li><code>doc</code>: The document ID ("trec_id" if you use ChatNoir or the official ClueWeb12 ID).</li>
		        <li><code>rel</code>: The relevance judgment: 0 (not relevant) to 3 (highly relevant). The quality judgment: 0 (low, or no arguments) to 3 (high).</li>
		</ul>
      	<p>You can use the corresponding <a href="./evaluate.py">evaluation script</a> for evaluation using the relevance judgments.</p>
	<!--
        SECTION Submission
        -->
        <h2 id="submission">Submission</h2>
        <p>We encourage participants to use <a href="https://www.tira.io/">TIRA</a> for their submissions to allow for a better reproducibility. Please also have a look at our <a href="#tira-quickstart">TIRA quickstart</a>&#8212;in case of problems we will be able to assist you. Even though the preferred way of run submission is TIRA, in case of problems you may also submit runs via email. We will try to quickly review your TIRA or email submissions and provide feedback.<br><br>
Runs may be either automatic or manual. An automatic run must not "manipulate" the topic titles via manual intervention. A manual run is anything that is not an automatic run. Upon submission, please let us know which of your runs are manual. For each topic, include up to 1,000 retrieved documents. Each team can submit <strong>up to 5</strong> different runs.</p>
                
                <p class="uk-text-left">
                        The submission format for the task will follow the standard TREC format:<br><br>
                        <code>qid stance doc rank score tag</code><br><br>

                        With:
                        <ul>
                                <li><code>qid</code>: The topic number.</li>
                                <li><code>stance</code>: The stance of the document (passage) toward the comparison objects (FIRST: pro 1st object, SECOND: pro 2nd object, NEUTRAL: neutral stance, NO: no stance).</li>
                                <li><code>doc</code>: The document (passage) ID <code>qid</code>.</li>
                                <li><code>rank</code>: The rank the document is retrieved at.</li>
                                <li><code>score</code>: The score (integer or floating point) that generated the ranking. The score must be in descending (non-increasing) order: it is important to handle tie scores.</li>
                                <li><code>tag</code>: A tag that identifies your group and the method you used to produce the run.</li>
                        </ul>
                        <p><strong>Note</strong>: If you do not classify the stance, use <code>Q0</code> as the value in the stance column.</p>

                                The fields should be separated by a whitespace. The individual columns' widths are not restricted (i.e., score can be an arbitrary precision that has no ties) but it is important to include all columns and to separate them with a whitespace.<br><br>

                                An example run for Task 2 is:<br><br>

                                <code>1 FIRST clueweb12-en0010-85-29836___1 1 17.89 myGroupMyMethod</code><br>
                                <code>1 NEUTRAL clueweb12-en0010-86-00457___3 2 16.43 myGroupMyMethod</code><br>
                                <code>1 NO clueweb12-en0010-86-09202___5 3 16.32 myGroupMyMethod</code><br>
                                <code>...</code><br></p>
        <p>We will provide an example of the submission format for stance detection until mid November.</p>
        <!--
        SECTION TIRA Quickstart
        -->
        <h2><a id="tira-quickstart"></a>TIRA Quickstart</h2>
        <p class=" uk-text-left">
            Participants have to upload (through SSH or RDP) their retrieval models in a dedicated TIRA virtual machine, so that their runs
            can be reproduced and so that they can be easily applied to different data (of same format) in the
            future. You can find host ports for your VM in the web interface, same login as to your VM.
            If you cannot connect to your VM, please make sure it is powered on: you can check and
            power on your machine in the web interface where you see the state of your VM and can access the connection informations on how to access your VM.
        </p>
        <img width="75%" style="margin-left: auto; margin-right: auto; display: block;" src="tira-task2-vm-overview.png" alt="Overview of the virtual machine in TIRA.">
        
        <p class=" uk-text-left">
            Your software is expected to accept two arguments:
            <ul>
                <li>An input directory (named <code>$inputDataset</code> in TIRA). This input directory contains a <code>topics.xml</code> file that contains the topics for which documents should be retrieved and a <code>passages.jsonl.gz</code> that contains the passages.</li>
                <li>An output directory (named <code>$outputDir</code> in TIRA). Your software should create a standard trec run file in <code>$outputDir/run.txt</code>.</li>
            </ul>
            As soon as your Software is installed in your VM, you can register it in TIRA.

            Assume that your software is started with a bash script in your home directory called <code>my-software.sh</code> which expects an argument <code>-i</code> specifying the input directory, and an argument <code>-o</code> specifying the output directory. Click on "Add software" and specify the command <code>./my-software.sh -i $inputDataset -o $outputDir</code>. Please select `touche-2022-task2` as input dataset.
        </p>
        <img width="75%" style="margin-left: auto; margin-right: auto; display: block;" src="tira-task2-example-software.png" alt="Overview of the software configuration in TIRA.">


        <p class=" uk-text-left">
            Please save your software and click on "Run" to execute your software in TIRA. Note that your VM
            will not be accessible while your system is running – it will be “sandboxed”, detached from the
            internet, and after the run the state of the VM before the run will be restored. Your run will be
            reviewed and evaluated by the organizers.
        </p>

        <p class=" uk-text-left font-italic">
            NOTE: By submitting your software you retain full copyrights. You agree to grant us usage rights for
            evaluation of the corresponding data generated by your software. We agree not to share your software with a
            third party or use it for any purpose other than research.
        </p>

        <p class=" uk-text-left">
            Once the run of your system completes, please also run
            the evaluator on the output of your system to verify that your output is a valid submission.
These are two separate actions and both should be invoked
            through the web interface of TIRA. You don’t have to install the evaluator in your VM. It is already
            prepared in TIRA. You can run the evaluator in the overview of your runs by clicking on the "Evaluate" button.
        </p>
        <img width="75%" style="margin-left: auto; margin-right: auto; display: block;" src="tira-task2-example-evaluation.png" alt="Overview of the software evaluation in TIRA.">
        <p class=" uk-text-left">
            By Clicking on "Inspect" you can see and download the STDOUT and STDERR as well as the outputs of your system.
The output of the evaluator will tell you if the output of your run valid.
If you think something went wrong with your run, send us an e-mail or open a <a href="https://www.tira.io/c/touche/9">new Topic in the Touché Forum in TIRA</a>.
Additionally, we review your submissions and contact you on demand.
            <br><br>
            You can register more than one system (“software/ model”) per virtual machine using the web interface.
            TIRA gives systems automatic names “Software 1”, “Software 2” etc. You can perform several runs per
            system.
        </p>


        <!--
        SECTION Results
        -->
        
	<h2 id="results">Results</h2>
	<h3>Results for Relevance Evaluation</h3>
	The table below shows the highest-scoring run of each team. The full table including all runs can be accessed <a class="uk-link" href="./data/task2_relevance_results_all.html">here</a>. Per-topic results are also available [<a href="./data/task2_relevance_results_full.csv">download</a>].
	<table class="uk-table uk-table-divider uk-table-small uk-table-hover">
  <thead>
    <tr style="text-align: right;">
      <th>Team</th>
      <th>Tag</th>
      <th>Mean nDCG@5</th>
      <th>CI95 Low</th>
      <th>CI95 High</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Captain Levi</td>
      <td>levirank_dense_initial_retrieval</td>
      <td>0.758</td>
      <td>0.708</td>
      <td>0.810</td>
    </tr>
    <tr>
      <td>Aldo Nadi</td>
      <td>seupd2122-kueri_rrf_reranked</td>
      <td>0.709</td>
      <td>0.649</td>
      <td>0.769</td>
    </tr>
    <tr>
      <td>Katana</td>
      <td>Colbert edinburg</td>
      <td>0.618</td>
      <td>0.551</td>
      <td>0.676</td>
    </tr>
    <tr>
      <td>Captain Tempesta</td>
      <td>hextech_run_1</td>
      <td>0.574</td>
      <td>0.504</td>
      <td>0.640</td>
    </tr>
    <tr>
      <td>Olivier Armstrong</td>
      <td>tfid_arg_similarity</td>
      <td>0.492</td>
      <td>0.414</td>
      <td>0.569</td>
    </tr>
    <tr>
      <td>Puss in Boots</td>
      <td>BM25-Baseline</td>
      <td>0.469</td>
      <td>0.403</td>
      <td>0.538</td>
    </tr>
    <tr>
      <td>Grimjack</td>
      <td>grimjack-fair-reranking-argumentative-axioms</td>
      <td>0.422</td>
      <td>0.351</td>
      <td>0.496</td>
    </tr>
    <tr>
      <td>Asuna</td>
      <td>asuna-run-5</td>
      <td>0.263</td>
      <td>0.202</td>
      <td>0.324</td>
    </tr>
  </tbody>
  </table>
 <h3>Results for Quality Evaluation</h3>
	The table below shows the highest-scoring run of each team. The full table including all runs can be accessed <a class="uk-link" href="./data/task2_quality_results_all.html">here</a>. Per-topic results are also available [<a href="./data/task2_quality_results_full.csv">download</a>].
	<table class="uk-table uk-table-divider uk-table-small uk-table-hover">
  <thead>
    <tr style="text-align: right;">
      <th>Team</th>
      <th>Tag</th>
      <th>Mean nDCG@5</th>
      <th>CI95 Low</th>
      <th>CI95 High</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Aldo Nadi</td>
      <td>seupd2122-kueri_RF_reranked</td>
      <td>0.774</td>
      <td>0.719</td>
      <td>0.828</td>
    </tr>
    <tr>
      <td>Captain Levi</td>
      <td>levirank_dense_initial_retrieval</td>
      <td>0.744</td>
      <td>0.690</td>
      <td>0.804</td>
    </tr>
    <tr>
      <td>Katana</td>
      <td>Colbert trained by me</td>
      <td>0.644</td>
      <td>0.575</td>
      <td>0.707</td>
    </tr>
    <tr>
      <td>Captain Tempesta</td>
      <td>hextech_run_5</td>
      <td>0.597</td>
      <td>0.523</td>
      <td>0.674</td>
    </tr>
    <tr>
      <td>Olivier Armstrong</td>
      <td>tfid_arg_similarity</td>
      <td>0.582</td>
      <td>0.506</td>
      <td>0.662</td>
    </tr>
    <tr>
      <td>Puss in Boots</td>
      <td>BM25-Baseline</td>
      <td>0.476</td>
      <td>0.401</td>
      <td>0.556</td>
    </tr>
    <tr>
      <td>Grimjack</td>
      <td>grimjack-fair-reranking-argumentative-axioms</td>
      <td>0.403</td>
      <td>0.330</td>
      <td>0.478</td>
    </tr>
    <tr>
      <td>Asuna</td>
      <td>asuna-run-5</td>
      <td>0.332</td>
      <td>0.254</td>
      <td>0.418</td>
    </tr>
  </tbody>
        </table>
         <h3>Results for Stance Prediction</h3>
	The table below shows the highest-scoring run of each team: <code>N_run, N_team:</code>number of stance predictions in a run and per team that were manually labeled. The full table including all runs can be accessed <a class="uk-link" href="./data/task2_stance_results_all.html">here</a>. Per-topic results are also available [<a href="./data/task2_stance_results_full.csv">download</a>].
	<table class="uk-table uk-table-divider uk-table-small uk-table-hover">
  <thead>
    <tr style="text-align: right;">
      <th>Team</th>
      <th>Tag</th>
      <th>F1_macro_run</th>
      <th>N_run</th>
      <th>F1_macro_team</th>
      <th>N_team</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Grimjack</td>
      <td>grimjack-all-you-need-is-t0</td>
      <td>0.313</td>
      <td>1208</td>
      <td>0.235</td>
      <td>1386</td>
    </tr>
    <tr>
      <td>Captain Levi</td>
      <td>levirank_dense_initial_retrieval</td>
      <td>0.301</td>
      <td>1688</td>
      <td>0.261</td>
      <td>2020</td>
    </tr>
    <tr>
      <td>Katana</td>
      <td>Colbert edinburg</td>
      <td>0.229</td>
      <td>1027</td>
      <td>0.220</td>
      <td>1301</td>
    </tr>
    <tr>
      <td>Olivier Armstrong</td>
      <td>tfid_arg_similarity</td>
      <td>0.191</td>
      <td>551</td>
      <td>0.191</td>
      <td>551</td>
    </tr>
    <tr>
      <td>Puss in Boots</td>
      <td>Always-NO-Baseline</td>
      <td>0.158</td>
      <td>1328</td>
      <td>0.158</td>
      <td>1328</td>
    </tr>
    <tr>
      <td>Asuna</td>
      <td>asuna-run-5</td>
      <td>0.106</td>
      <td>578</td>
      <td>0.106</td>
      <td>578</td>
    </tr>
  </tbody>
  </table>

	<h3>Qrels Download (top-5 pooling)</h3>
	<ul>
	    <li>Relevance qrels [<a href="https://files.webis.de/corpora/corpora-webis/corpus-touche-task2-22/touche-task2-2022-relevance.qrels">download</a>]</li>
	    <li>Quality qrels [<a href="https://files.webis.de/corpora/corpora-webis/corpus-touche-task2-22/touche-task2-2022-quality.qrels">download</a>]</li>
	    <li>Stance labels [<a href="https://files.webis.de/corpora/corpora-webis/corpus-touche-task2-22/touche-task2-2022-stance.qrels">download</a>]</li>
        </ul>       
        

        <!--
        SECTION Task Committee
        -->
        <h2><a id="task-committee"></a>Task Committee</h2>
        <div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
            {% include people-cards/bondarenko.html gender="male" %}
            {% include people-cards/hagen.html gender="male" %}
            {% include people-cards/froebe.html gender="male" %}
            {% include people-cards/beloucif.html gender="female" %}
            {% include people-cards/biemann.html gender="male" %}
            {% include people-cards/panchenko.html gender="male" %}
        </div>

    </div>
</main>

<script src="https://assets.webis.de/js/filter.js"></script>
