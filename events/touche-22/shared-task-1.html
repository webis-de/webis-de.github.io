---
layout: workshop
nav_active: shared task 1
title: Touché @ CLEF
description: Touché @ CLEF Argument Retrieval
shared_tasks: 3
---
<nav class="uk-container">
    <ul class="uk-breadcrumb">
        <li><a href="https://webis.de/index.html">Webis.de</a></li>
        <li><a href="https://webis.de/events.html">Events</a></li>
        <li><a href="index.html">Touché 2022</a></li>
        <li class="uk-disabled"><a href="#">Task 1</a></li>
    </ul>
</nav>

<script type="application/ld+json">
    {
        "@context": "http://schema.org/",
        "@type": "Shared Task",
        "name": "Touché @ CLEF",
        "description": "Touché @ CLEF Argument Retrieval",
        "text": "*Shared Task, any other text",
        "url": "*link that leads to this HTML web page.",
        "sameAs": "*is there an id link, e.g. from zenodo?",
        "keywords": [
            "touché",
            "argument retrieval",
            "argument retrieval for controversial questions",
            "argument retrieval for comparative questions"
        ],
        "creator": [{
                "@type": "Organization",
                "url": "https://webis.de/",
                "name": "The Web Technology & Information Systems Network",
                "alternateName": "Webis"
            },
            {
                "@id": "*orcid id if available",
                "@type": "Person",
                "url": "*https://www.uni-weimar.de/en/media/chairs/computer-science-department/webis/people/#name",
                "affiliation": "*Bauhaus-Universit\u00e4t Weimar",
                "name": "*last name, first name"
            },
            {
                "@id": "*orcid id if available",
                "@type": "Person",
                "url": "*https://www.uni-weimar.de/en/media/chairs/computer-science-department/webis/people/#name",
                "affiliation": "*Bauhaus-Universit\u00e4t Weimar",
                "name": "*last name, first name"
            }
        ],
        "includedInDataCatalog": {},
        "distribution": []
    }
</script>

<main class="uk-section uk-section-default">
    <div class="uk-container">
        <h1>Touché Task 1: Argument Retrieval for Controversial Questions</h1>

        <ul class="uk-list">
            <!-- Comment out sections you do not provide -->
            <li><span data-uk-icon="chevron-down"></span> <a href="#synopsis">Synopsis</a></li>
            <li><span data-uk-icon="chevron-down"></span> <a href="#task">Task</a></li>
            <li><span data-uk-icon="chevron-down"></span> <a href="#data">Data</a></li>
            <li><span data-uk-icon="chevron-down"></span> <a href="#evaluation">Evaluation</a></li>
            <li><span data-uk-icon="chevron-down"></span> <a href="#submission">Submission</a></li>
            <li><span data-uk-icon="chevron-down"></span> <a href="#tira-quickstart">TIRA Quickstart</a></li>
            <!-- <li><span data-uk-icon="chevron-down"></span> <a href="#results">Results</a></li> -->
            <li><span data-uk-icon="chevron-down"></span> <a href="#task-committee">Task Committee</a></li>
        </ul>
    </div>

    <div class="uk-container uk-margin-medium">
        <!--
        SECTION Synopsis
        -->
        <h2 id="synopsis">Synopsis</h2>

        <ul>
            <li>Task: Given a query about a controversial topic, retrieve and rank a relevant pair of sentences from a
                collection of arguments.</li>
            <li>Input: [<a
                    href="https://files.webis.de/data-in-progress/data-research/arguana/touche/touche22/">data</a>]</li>
            <li>Submission: [<a href="https://www.tira.io/task/touche-task-1/">submit</a>]</li>
        </ul>

        <!--<p>You may provide links to associated resources, like so: [<a>link</a>]</p> -->


        <!--
        SECTION Task
        -->
        <h2><a id="task"></a>Task</h2>

        <p>The goal of <strong>Task 1</strong> is to support users who search for arguments to be used in conversations
            (e.g., getting an overview of pros and cons or just looking for arguments in line with a user's stance).
            However, in contrast to previous iterations of this task, we now require <strong>retrieving a pair of
                sentences</strong> from a collection of arguments. Each sentence in this pair must be argumentative
            (e.g., a claim, a supporting premise, or a conclusion). Also, to encourage diversity of the retrieved
            results, sentences in this pair may come from two different arguments.</p>

        <a class="uk-button uk-button-primary" href="https://clef2022-labs-registration.dei.unipd.it/">Register now</a>

        <!--
        SECTION Data
        -->
        <h2><a id="data"></a>Data</h2>

        <p><strong>The corpus</strong> for Task 1 (available <a
                href="https://files.webis.de/data-in-progress/data-research/arguana/touche/touche22/">here</a>) is a
            pre-processed version of the <a href="https://zenodo.org/record/3734893">args.me corpus (version
                2020-04-01)</a> where each argument is split into sentences; you may index these sentences (and the
            complete arguments if you wish to) with your favorite retrieval system. To ease participation, you may also
            directly use the <a href="https://www.args.me/index.html">args.me</a> search engine's <a
                href="https://www.args.me/api-en.html">API</a> for a baseline retrieval and then extract the candidate
            pair of sentences.</p>

        <h4>Optional Data</h4> <br>
        Participants can access the query relevance judgments and quality judgments of the past edition of this task
        (where complete arguments were retrieved instead of sentence pairs). This data may serve as a source of distant
        supervision, but there is no strict requirement to harness it.

        <p><strong>Touché 2021 topics (only titles)</strong> for Task 1 can be <a
                href="./topics-task-1-only-titles-2021.zip">downloaded from here</a>. The topics are provided as an XML
            file.<br><br>
            Example topic:</strong><br><br>
            &nbsp;&nbsp;&nbsp;&#60;topic&#62;<br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#60;number&#62;1&#60;/number&#62;<br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#60;title&#62;Is human activity primarily responsible for global
            climate change?&#60;/title&#62;<br>
            <!--  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#60;description&#62;As the evidence that the climate is changing rapidly mounts, a user questions the common belief that climate change is anthropogenic and desires to know whether humans are the primary cause, or whether there are other causes.&#60;/description&#62;<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#60;narrative&#62;Highly relevant arguments include those that take a stance in favor of or opposed to climate change being anthropogenic and that offer valid reasons for either stance. Relevant arguments talk about human or non-human causes, but not about primary causes. Irrelevant arguments include ones that deny climate change.&#60;/narrative&#62;<br> -->
            &nbsp;&nbsp;&nbsp;&#60;/topic&#62;<br><br> </p>

        [<a href="./touche-task1-51-100-relevance.qrels">Download relevance judgments from Touché 2021 (args.me corpus
            version 2020-04-01)</a>]<br>
        [<a href="./touche-task1-51-100-quality.qrels">Download quality judgments from Touché 2021 (args.me corpus
            version 2020-04-01)</a>]<br>

        <!-- <h3>Touché 2020 Results</h3>
                        
        [<a href="./topics-task-1-2020.zip">Download full topics from Touché 2020</a>] <br>
        [<a href="./touche2020-task1-relevance-args-me-corpus-version-1.qrels">Download relevance judgments from Touché 2020 (args.me corpus version 1)</a>]<br>
        [<a href="./touche2020-task1-relevance-args-me-corpus-version-2020-04-01-corrected.qrels">Download <strong>corrected</strong> relevance judgments from Touché 2020 (args.me corpus version 2020-04-01)</a>]<br>
        [<a href="./runs-task-1-2020.zip">Download submitted ranked documents (runs) from Touché 2020</a>]<br><br>

         -->

        <!--
        <h3 class="uk-margin-small-top">Output Format</h3>
        <p>Output format description.</p>
	-->

        <!--
        SECTION Evaluation
        -->
        <h2><a id="evaluation"></a>Evaluation</h2>
        <p>Be sure to retrieve a pair of ''strong'' argumentative sentences. Our human assessors will label the
            retrieved pairs manually, both for their general topical relevance and for their argument quality, i.e.,:
            (1) whether each sentence in the retrieved pair is argumentative (claim, supporting premise, or conclusion),
            (2) whether the sentence pair forms a coherent text (sentences in a pair must not contradict each other),
            (3) whether the sentence pair forms a short summary of the corresponding arguments from which these
            sentences come from; each sentence in the pair must ideally be the most representative / most important
            sentence of its corresponding argument.</p>

        The format of the relevance/quality judgment file:<br><br>
        <code>qid 0 pair rel</code><br><br>
        With:
        <ul>
            <li><code>qid</code>: The topic number.</li>
            <li><code>0</code>: Unused, always 0.</li>
            <li><code>pair</code>: The pair of sentence IDs (from the provided version of the args.me corpus).</li>
            <li><code>rel</code>: The relevance judgment: -2 non-argument (spam), 0 (not relevant) to 3 (highly
                relevant). The quality judgment: 0 (low) to 3 (high).</li>
        </ul>
        <!-- <p>You can use the corresponding <a href="./evaluate.py">evaluation script</a> to evaluate your run using the relevance judgments.</p> -->

        <!--
        SECTION Submission
        -->
        <h2><a id="submission"></a>Submission</h2>
        <p>We encourage participants to use <a href="https://www.tira.io/">TIRA</a> for their submissions to allow for a
            better reproducibility. Please also have a look at our <a href="#tira-quickstart">TIRA
                quickstart</a>&#8212;in case of problems we will be able to assist you. Even though the preferred way of
            run submission is TIRA, in case of problems you may also submit runs via email. We will try to quickly
            review your TIRA or email submissions and provide feedback.<br><br>
            Runs may be either automatic or manual. An automatic run must not "manipulate" the topic titles via manual
            intervention. A manual run is anything that is not an automatic run. Upon submission, please let us know
            which of your runs are manual. For each topic, include a minimum of 100 and up to 1000 retrieved sentence
            pairs. Each team can submit <strong>up to 5</strong> different runs.
        </p>
        <p class="uk-text-left">
            The submission format for the task will follow the standard TREC format:<br><br>
            <code>qid stance pair rank score tag</code><br><br>

            With:
            <ul>
                <li><code>qid</code>: The topic number.</li>
                <li><code>stance</code>: The stance of the sentence pair ("PRO" or "CON").</li>
                <!-- <li><code>Q0</code>: Unused, should always be Q0.</li> -->
                <li><code>pair</code>: The pair of sentence IDs (from the provided version of the args.me corpus)
                    returned by your system for the topic <code>qid</code>.</li>
                <li><code>rank</code>: The rank the document is retrieved at.</li>
                <li><code>score</code>: The score (integer or floating point) that generated the ranking. The score must
                    be in descending (non-increasing) order: it is important to handle tie scores.</li>
                <li><code>tag</code>: A tag that identifies your group and the method you used to produce the run.</li>
            </ul>

            The fields should be separated by a whitespace. The individual columns' widths are not restricted (i.e.,
            score can be an arbitrary precision that has no ties) but it is important to include all columns and to
            separate them with a whitespace.<br><br>

            An example run for Task 1 is:<br><br>

            <code>1 PRO S71152e5e-A66163a57__CONC__1,S71152e5e-A66163a57__PREMISE__2 1 17.89 myGroupMyMethod</code><br>
            <code>1 PRO S6c286161-Aafd7e261__CONC__1,S6c286161-Aafd7e261__PREMISE__1 2 16.43 myGroupMyMethod</code><br>
            <code>1 CON S72f5af83-Afb975dba__PREMISE__1,S72f5af83-Afb975dba__CONC__1 3 16.42 myGroupMyMethod</code><br>
            <code>...</code>
            <br>
            <strong>Note:</strong> If you do not have stance information, use <code>Q0</code> as the value in the stance
            column.
        </p>


        <!--
SECTION TIRA Quickstart
-->
<h2><a id="tira-quickstart"></a>TIRA Quickstart</h2>
<p>
Participant software is run in a virtual machine. Log in to <a href="https://www.tira.io">TIRA</a>, go to the <a href="https://www.tira.io/task/touche-task-1/dataset/touche-task-1-2022-02-16">task's dataset page</a>, and click on "&gt;_ SUBMIT". Click the "CONNECTION INFO" button for how to connect to the virtual machine. Click on "POWER ON" if the state is not "RUNNING". 
</p>
<img width="75%" src="tira-task1-vm-overview.png" alt="Virtual machine state in TIRA.">
<p>
The software is executed on the command line with two parameters: (1) <code>$inputDataset</code> refers to a directory that contains the <a href="#data">collection</a>; (2) <code>$outputDir</code> refers to a directory in which the software has to create the <a href="#submission">submission file</a> named <code>run.txt</code>. Specify exactly how each software of your virtual machine is run using the "Command" field in the TIRA web interface. Select <code>touche-task-1-2022-02-16</code> as the input dataset.
</p>
<img width="75%" src="tira-task1-example-software.png" alt="Software configuration in TIRA.">
<p>
As you "RUN" the software, you will not be able to connect to the virtual machine (takes at least 10 minutes). Once finished, click on "INSPECT" to check on the run and click on "EVALUATE" for a syntax check (give it a few minutes, then check back on the page). Your run will later be reviewed and evaluated by the organizers. If uncertain on something, ask in the <a href="https://www.tira.io/c/touche/9">forum</a> or send a mail/message to <a href="#task-committee">Shahbaz</a>.
</p>
<img width="75%" src="tira-task1-example-evaluation.png" alt="A run in TIRA.">
<p>

Create a separate "Software" entry in the TIRA web interface for each of your approaches. NOTE: By submitting your software you retain full copyrights. You agree to grant us usage rights for evaluation of the corresponding data generated by your software. We agree not to share your software with a third party or use it for any purpose other than research.
</p>

        <!--
        SECTION Results
        -->

        <!--
        SECTION Task Committee
        -->
        <h2><a id="task-committee"></a>Task Committee</h2>
        <div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
            {% include people-cards/syed.html gender="male" %}
            {% include people-cards/gurcke.html gender="male" %}
            {% include people-cards/bondarenko.html gender="male" %}
            {% include people-cards/froebe.html gender="male" %}
            {% include people-cards/hagen.html gender="male" %}
            {% include people-cards/stein.html gender="male" %}
            {% include people-cards/wachsmuth.html gender="male" %}
            {% include people-cards/potthast.html gender="male" %}
        </div>

    </div>
</main>

<script src="https://assets.webis.de/js/filter.js"></script>