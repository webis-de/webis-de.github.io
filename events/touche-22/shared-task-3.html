---
layout: workshop
nav_active: shared task 3
title: Touché @ CLEF
description: Touché @ CLEF Argument Retrieval
shared_tasks: 3
---
<nav class="uk-container">
<ul class="uk-breadcrumb">
<li><a href="https://webis.de/index.html">Webis.de</a></li>
<li><a href="https://webis.de/events.html">Events</a></li>
<li><a href="index.html">Touché 2022</a></li>
<li class="uk-disabled"><a href="#">Task 3</a></li>
</ul>
</nav>

<script type="application/ld+json">
{
    "@context":"http://schema.org/",
    "@type":"Shared Task",
    "name":"Touché @ CLEF",
    "description":"Touché @ CLEF Argument Retrieval",
    "url":"*link that leads to this HTML web page.",
    "keywords":[
        "touché", 
        "argument retrieval", 
        "image retrieval"
    ], 
    "creator":[
        {
            "@type":"Organization",
            "url":"https://webis.de/",
            "name":"The Web Technology & Information Systems Network",
            "alternateName":"Webis"
        }
    ]
}
</script>

<main class="uk-section uk-section-default">
<div class="uk-container">
<h1>Touché Task 3: Image Retrieval for Arguments</h1>

<ul class="uk-list">
<!-- Comment out sections you do not provide -->
<li><span data-uk-icon="chevron-down"></span> <a href="#synopsis">Synopsis</a></li>
<li><span data-uk-icon="chevron-down"></span> <a href="#data">Data</a></li>
<li><span data-uk-icon="chevron-down"></span> <a href="#evaluation">Evaluation</a></li>
<li><span data-uk-icon="chevron-down"></span> <a href="#submission">Submission</a></li>
<li><span data-uk-icon="chevron-down"></span> <a href="#tira-quickstart">TIRA Quickstart</a></li>
<!-- <li><span data-uk-icon="chevron-down"></span> <a href="#results">Results</a></li> -->
<li><span data-uk-icon="chevron-down"></span> <a href="#related-work">Related Work</a></li>
<li><span data-uk-icon="chevron-down"></span> <a href="#task-committee">Task Committee</a></li>
</ul>
</div>

<div class="uk-container uk-margin-medium">
<!--
SECTION Synopsis
-->
<h2 id="synopsis">Synopsis</h2>
<ul>
  <li>Task: Given a controversial topic, the task is to retrieve images (from web pages) for each stance (pro/con) that show support for that stance. [<a href="https://www.tira.io/c/touche/9">forum</a>] [<a href="https://www.tira.io/t/announcements-for-task-3-image-retrieval-for-arguments-2022/556">announcements</a>]</li>
<li>Input: [<a href="https://files.webis.de/corpora/corpora-webis/corpus-touche-image-search-22/">collection</a>] [<a href="https://files.webis.de/corpora/corpora-webis/corpus-touche-image-search-22/training-qrels.txt">training judgements</a>]</li>
<li>Evaluation: [<a href="https://files.webis.de/corpora/corpora-webis/corpus-touche-image-search-22/topics.xml">topics</a>]</li>
<li>Submission: [<a href="https://files.webis.de/corpora/corpora-webis/corpus-touche-image-search-22/original-rankings-baseline.py">baseline</a>] [verifier] [tira] (coming soon)</li>
</ul>

<a class="uk-button uk-button-primary" href="https://clef2022-labs-registration.dei.unipd.it/">Register now</a>

<!--
SECTION Data
-->
<h2><a id="data"></a>Data</h2>
<p>
This task uses a focused crawl of about 20,000 images (and associated web pages) as document collection. See the collection's <a href="https://files.webis.de/corpora/corpora-webis/corpus-touche-image-search-22/README.txt">README</a> for more information on its contents and file formats. We may still modify the collection until the end of 2021 based on feedback and own considerations. The collection also contains a small set of training judgements for basic testing.
[<a href="https://files.webis.de/corpora/corpora-webis/corpus-touche-image-search-22/">collection</a>] [<a href="https://files.webis.de/corpora/corpora-webis/corpus-touche-image-search-22/training-qrels.txt">training judgements</a>]
</p>

<!--
SECTION Evaluation
-->
<h2><a id="evaluation"></a>Evaluation</h2>
<p>
Systems are evaluated on Touché topics 1–50 by the ratio of relevant images among 20 retrieved images (Precision) for each topic, namely 10 images for each stance (file format explained in the <a href="https://files.webis.de/corpora/corpora-webis/corpus-touche-image-search-22/README.txt">README</a>). [<a href="https://files.webis.de/corpora/corpora-webis/corpus-touche-image-search-22/topics.xml">topics</a>]
</p>

<!--
SECTION Submission
-->
<h2><a id="submission"></a>Submission</h2>
<p>
We encourage participants to use <a href="https://www.tira.io/">TIRA</a> for their submissions to allow for a better reproducibility (see the Quickstart section below). Email submission is allowed as a fallback. For each topic and stance, include 10 retrieved images. Each team can submit up to 5 different runs.
</p><p>
The submission format adapts the standard TREC format. Each line corresponds to an image retrieved for some topic and stance at a certain rank, making a run file 1000 lines long (50 topics, 2 stances, 10 ranks). Each line contains the following fields, separated by single whitespaces:
</p>
<ul>
<li>The topic number (1 to 50).</li>
<li>The stance ("PRO" or "CON").</li>
<li>The image's ID (corresponds to the name of the image's directory in the collection; always 17 characters long and starts with "I").</li>
<li>The rank (1 to 10 in increasing order per topic and stance). Not used in this year's evaluation.</li>
<li>A score (integer or floating point; non-increasing per topic and stance). Not used in this year's evaluation.</li>
<li>A tag that identifies your group and the method you used to produce the run.</li>
</ul>
For example:
<pre>
1 PRO I000330ba4ea0ad13 1 17.89 myGroupMyMethod
1 PRO I0005e6fe00ea17fd 2 16.43 myGroupMyMethod
...
1 CON I0009d5f038fe6f2e 1 15.89 myGroupMyMethod
1 CON I000f34bd3f8cb030 2 14.43 myGroupMyMethod
...
</pre>
<p>
If you have questions, please ask in the <a href="https://www.tira.io/c/touche/9">forum</a>. You will get a combined TIRA-and-forum account on registration. Announcements are published <a href="https://www.tira.io/t/announcements-for-task-3-image-retrieval-for-arguments-2022/556">here</a>.
</p>

<!--
SECTION TIRA Quickstart
-->
<h2><a id="tira-quickstart"></a>TIRA Quickstart</h2>
Coming soon
<!--
<p>
Participants have to upload (through SSH or RDP) their retrieval models in a dedicated TIRA virtual machine, so that their runs can be reproduced and so that they can be easily applied to different data (of same format) in the future. You can find host ports for your VM in the web interface, same login as to your VM.  If you cannot connect to your VM, please make sure it is powered on: you can check and power on your machine in the web interface.
</p>
<p>
Your software is expected to accept two arguments:
<ul>
<li>An input directory (named <code>$inputDataset</code> in TIRA). The variable <code>$inputDataset</code> points to a directory which contains six files: <code>debateorg.json</code>, <code>debatepedia.json</code>, <code>debatewise.json</code>, <code>idebate.json</code>, <code>parliamentary.json</code> (those 5 files form the corpus), and <code>topics.xml</code> (the topics for which documents should be retrieved).</li>
<li>An output directory (named <code>$outputDir</code> in TIRA). Your software should create a standard trec run file in <code>$outputDir/run.txt</code>.</li>
</ul>
Your Software can use the <code>args-me.json</code> file or the API of the search engine <a href="https://www.args.me/api-en.html">args.me</a> to produce the run file.<br>
As soon as your Software is installed in your VM, you can register it in TIRA.

Assume that your software is started with a bash script in your home directory called <code>my-software.sh</code> which expects an argument <code>-i</code> specifying the input directory, and an argument <code>-o</code> specifying the output directory. Click on "Add software" and specify the command <code>./my-software.sh -i $inputDataset -o $outputDir</code>. The other fields can stay with default settings.
</p>
<img width="75%" style="margin-left: auto; margin-right: auto; display: block;" src="tira-task1-example-software.png" alt="Overview of the software configuration in TIRA.">


<p class=" uk-text-left">
Click on "Run" to execute your software in TIRA. Note that your VM
will not be accessible while your system is running – it will be “sandboxed”, detached from the
internet, and after the run the state of the VM before the run will be restored. Your run will be
reviewed and evaluated by the organizers.
</p>

<p class=" uk-text-left font-italic">
NOTE: By submitting your software you retain full copyrights. You agree to grant us usage rights for
evaluation of the corresponding data generated by your software. We agree not to share your software with a
third party or use it for any purpose other than research.
</p>

<p class=" uk-text-left">
Once the run of your system completes, please also run
the evaluator on the output of your system to verify that your output is a valid submission.
These are two separate actions and both should be invoked
through the web interface of TIRA. You don’t have to install the evaluator in your VM. It is already
prepared in TIRA. You should see it in the web interface, under your software, labeled “Evaluator”.
Before clicking the “Run” button, you will use a drop-down menu to select the “Input run”, i.e. one of
the completed runs of your system. The output files from the selected run will be evaluated.
</p>
<img width="75%" style="margin-left: auto; margin-right: auto; display: block;" src="tira-task1-example-evaluation.png" alt="Overview of the software evaluation in TIRA.">
<p class=" uk-text-left">
You can see and download STDOUT and STDERR as well as the outputs of your system.
In the evaluator run you will see only STDOUT and STDERR, which will tell you if one or more of your output files is not valid.
If you think something went wrong with your run, send us an e-mail.
Additionally, we review your submissions and contact you on demand.
<br><br>
You can register more than one system (“software/ model”) per virtual machine using the web interface.
TIRA gives systems automatic names “Software 1”, “Software 2” etc. You can perform several runs per
system.
</p>
-->


<!--
SECTION Task Committee
-->
<h2 id="related-work">Related Work</h2>
<ul>
<li>
Johannes Kiesel, Nico Reichenbach, Benno Stein, and Martin Potthast.
<a href="https://webis.de/publications.html#kiesel_2021e">Image Retrieval for Arguments Using Stance-Aware Query Expansion.</a>
8th Workshop on Argument Mining (ArgMining 2021) at EMNLP, November 2021.
</li>
<li>
Dimitar Dimitrov, Bishr Bin Ali, Shaden Shaar, Firoj Alam, Fabrizio Silvestri, Hamed Firooz, Preslav Nakov, and Giovanni Da San Martino.
<a href="https://aclanthology.org/2021.semeval-1.7/">SemEval-2021 Task 6: Detection of Persuasion Techniques in Texts and Images</a>.
15th International Workshop on Semantic Evaluation (SemEval 2021), August 2021.
</li>
<li>
Keiji Yanai.
<a href="https://dl.acm.org/doi/10.1145/1242572.1242816">Image collector III: a web image-gathering system with bag-of-keypoints.</a>
16th International Conference on World Wide Web (WWW 2007), May 2007.
</li>
</ul>


<!--
SECTION Task Committee
-->
<h2><a id="task-committee"></a>Task Committee</h2>
<div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
{% include people-cards/kiesel.html gender="male" %}
{% include people-cards/potthast.html gender="male" %}
{% include people-cards/stein.html gender="male" %}
</div>

</div>
</main>

<script src="https://assets.webis.de/js/filter.js"></script>
